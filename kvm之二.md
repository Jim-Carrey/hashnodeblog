---
title: KVM 之二：KVM原理简介
domain: www.jimcarrey.fun
tags: github
subtitle: KVM原理简介
slug: kvm-introduction
cover: https://sm.ms/image/m9OqBbgszi7y6Df
---

# 硬件虚拟化技术

KVM虚拟化必须依赖于硬件辅助的虚拟化技术。最早的硬件虚拟化技术出现在 1972 年的大型机IBM System/370 系统上，而真正让硬件虚拟化技术“走入寻常百姓家”的是 2005 年年末 Intel 发布的 VT-x 硬件虚拟化技术，以及 AMD 于2006年发布的 AMD-V。下面将以 Intel 的硬件虚拟化技术作为代表来介绍。

## CPU虚拟化

一个实际的物理 CPU 核是无法切割成两份的，也无法凭空诞生出另一个CPU，那 CPU 的虚拟化又是什么呢？又是如何做到？

说白了，虚拟化所谓的'虚拟'，即表示这个东西本身是物理不存在的，但是可以制造一种假象，这种假象就是让你认为你'实际'是拥有一个 CPU 核心，并且是独占。这跟之前应用程序和操作系统的关系差不多，N年前的大型机，其上的操作系统，或者说根本就不能称之为操作系统，因为它是单任务的，同一时间只能运行一个作业程序，只能等上一个任务运行结束了，才能放入下一个任务。

这便促成了现代操作系统的诞生，为啥在现在的操作系统之上可以'同时'运行多个应用程序呢？这便是操作系统给上层提供的假象，OS 用调度轮转的策略，分别调度运行不同的作业程序(其实单核下同一时间也只有一个程序在运行而已)，这便让每个程序都认为--它是独占整个 CPU 核心的，这其实也跟虚拟非常类似。

所以要想虚拟，负责虚拟的一方必须对整个环境有更高权限的控制，一般 OS 所处的 CPU 环境权限就高于应用程序，这样处于权限更大一层的一方才能制造'假象'，迷惑被虚拟化的一方。

通常我们的常识是：一个物理机器上(这里我们不考虑异构环境[即 SMP 环境下不同的核心 run 不同的 OS]，我们只说单核)只能 run 一个 OS，那么如何在仅有的这个核心上可以 run 不同的 OS呢？ 这便需要比操作系统运行环境更高一级的一种'特殊软件'来制造这种'假象'，以此来迷惑 OS，让 OS 认为它是独占这个 CPU 核心的。

这个比操作系统运行环境权限更高的一种特殊的软件就叫做**虚拟机监控器**(Hypervisor)，其运行在硬件 CPU 为其专门提供的更高一级权限的环境当中。如：

Intel x86：Hypervisor 运行在根模式，客户机在非根模式

ARM ：Hypervisor 运行在 EL2，客户机在 EL1(应用程序在EL0)

低权限环境的程序执行一些敏感指令时，其必须要陷入更高级权限环境中，由更高级权限环境所处的系统接管处理，处理完之后再返回到低权限环境的代码中执行。

下面将以 Intel 的硬件虚拟化技术作为代表来介绍。

CPU是计算机系统最核心的模块，我们的程序执行到最后都是翻译为机器语言在CPU上执行的。在没有CPU硬件虚拟化技术之前，通常使用指令的二进制翻译（binary translation）来实现虚拟客户机中CPU指令的执行，很早期的VMware就使用这样的方案，其指令执行的翻译比较复杂，效率比较低。所以Intel最早发布的虚拟化技术就是CPU虚拟化方面的，这才为本书的主角—KVM的出现创造了必要的硬件条件。

Intel在处理器级别提供了对虚拟化技术的支持，被称为VMX（virtual-machine extensions）。有两种VMX操作模式：VMX 根操作（root operation） 与VMX 非根操作（non-root operation）。作为虚拟机监控器中的KVM就是运行在根操作模式下，而虚拟机客户机的整个软件栈（包括操作系统和应用程序）则运行在非根操作模式下。进入VMX非根操作模式被称为“VM Entry”；从非根操作模式退出，被称为“VM Exit”。

VMX的根操作模式与非VMX模式下最初的处理器执行模式基本一样，只是它现在支持了新的VMX相关的指令集以及一些对相关控制寄存器的操作。VMX的非根操作模式是一个相对受限的执行环境，为了适应虚拟化而专门做了一定的修改；在客户机中执行的一些特殊的敏感指令或者一些异常会触发“VM Exit”退到虚拟机监控器中，从而运行在VMX根模式。正是这样的限制，让虚拟机监控器保持了对处理器资源的控制。

## 内存虚拟化

内存虚拟化的目的是给虚拟客户机操作系统提供一个从0地址开始的连续物理内存空间，同时在多个客户机之间实现隔离和调度。在虚拟化环境中，内存地址的访问会主要涉及以下4个基础概念，图2-2形象地展示了虚拟化环境中内存地址。

![image.png](https://s2.loli.net/2022/06/22/a52icGBl4PJofM8.png)

1）客户机虚拟地址，GVA（Guest Virtual Address）
2）客户机物理地址，GPA（Guest Physical Address）
3）宿主机虚拟地址，HVA（Host Virtual Address）
4）宿主机物理地址，HPA（Host Physical Address）
内存虚拟化就是要将客户机虚拟地址（GVA）转化为最终能够访问的宿主机上的物理地址（HPA）。对于客户机操作系统而言，它不感知内存虚拟化的存在，在程序访问客户机中虚拟地址时，通过CR3寄存器可以将其转化为物理地址，但是在虚拟化环境中这个物理地址只是客户机的物理地址，还不是真实内存硬件上的物理地址。所以，虚拟机监控器就需要维护从客户机虚拟地址到宿主机物理地址之间的一个映射关系，在没有硬件提供的内存虚拟化之前，这个维护映射关系的页表叫作影子页表（Shadow Page Table）。内存的访问和更新通常是非常频繁的，要维护影子页表中对应关系会非常复杂，开销也较大。同时需要为每一个客户机都维护一份影子页表，当客户机数量较多时，其影子页表占用的内存较大也会是一个问题。
Intel CPU 在硬件设计上就引入了EPT（Extended Page Tables，扩展页表），从而将客户机虚拟地址到宿主机物理地址的转换通过硬件来实现。当然，这个转换是通过两个步骤来实现的，如图2-3所示。首先，通过客户机 CR3 寄存器将客户机虚拟地址转化为客户机物理地址，然后通过查询 EPT 来实现客户机物理地址到宿主机物理地址的转化。EPT 的控制权在虚拟机监控器中，只有当 CPU工作在非根模式时才参与内存地址的转换。使用 EPT后，客户机在读写 CR3 和执行INVLPG 指令时不会导致 VM Exit，而且客户页表结构自身导致的页故障也不会导致 VM Exit。所以通过引入硬件上 EPT 的支持，简化了内存虚拟化的实现复杂度，同时也提高了内存地址转换的效率。

![image.png](https://s2.loli.net/2022/06/22/YaTlJMKy1gbecRt.png)

除了EPT，Intel 在内存虚拟化效率方面还引入了 VPID（Virtual-processor identifier）特性，在硬件级对 TLB 资源管理进行了优化。在没有 VPID 之前，不同客户机的逻辑 CPU 在切换执行时需要刷新 TLB，而 TLB 的刷新会让内存访问的效率下降。VPID 技术通过在硬件上为 TLB 增加一个标志，可以识别不同的虚拟处理器的地址空间，所以系统可以区分虚拟机监控器和不同虚拟机上不同处理器的 TLB，在逻辑 CPU 切换执行时就不会刷新 TLB，而只需要使用对应的 TLB 即可。VPID 的示意图如图2-4所示。当 CPU 运行在非根模式下，且虚拟机执行控制寄存器的 “enable VPID” 比特位被置为 1 时，当前的 VPID 的值是 VMCS 中的 VPID 执行控制域的值，其值是非 0 的。VPID 的值在 3 种情况下为 0，第 1 种是在非虚拟化环境中执行时，第 2 种是在根模式下执行时，第 3 种情况是在非根模式下执行但 “enable VPID” 控制位被置 0 时。



## I/O虚拟化

# kvm原理简介

## 1.kvm

### kvm 是一种基于内核的虚拟监控器的实现(Kernel-based Virtual Machine)

kvm 依赖硬件虚拟化的支持，其作为一个内核模块，加载到 linux 中后使 linux 变为 type2 型的 hypervisor 。linux 2.16 版本后，kvm 已被并入 linux 主线。

kvm 模块在内核中主要分为两部分，一部分是与处理器架构无关的 kvm 模块，另一部分是与处理器虚拟化相关的部分，如在 intel 平台上其为 kvm_intel 模块。

KVM的主要功能是初始化CPU硬件，打开虚拟化模式，然后将虚拟客户机运行在虚拟机模式下，并对虚拟客户机的运行提供一定的支持。

### KVM虚拟化核心

1）KVM内核模块，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的模块，主要负责 CPU 和内存的虚拟化，包括：客户机的创建、虚拟内存的分配、CPU 执行模式的切换、vCPU寄存器的访问、vCPU的执行。
2）QEMU用户态工具，它是一个普通的 Linux 进程，为客户机提供设备模拟的功能，包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。同时它通过ioctl系统调用与内核态的 KVM 模块进行交互。
KVM是在硬件虚拟化支持下的完全虚拟化技术，所以它能支持在相应硬件上能运行的几乎所有的操作系统，如：Linux、Windows、FreeBSD、MacOS等。

KVM的基础架构如图2-8所示
![image.png](https://s2.loli.net/2022/06/22/idtcRzCh8frAPYG.png)

在KVM虚拟化架构下，每个客户机就是一个QEMU进程，在一个宿主机上有多少个虚拟机就会有多少个QEMU进程；客户机中的每一个虚拟CPU对应QEMU进程中的一个执行线程；一个宿主机中只有一个KVM内核模块，所有客户机都与这个内核模块进行交互。

### 工作流程举例

以 KVM 在 Intel 公司的 CPU上运行为例，在被内核加载的时候，KVM 模块会先初始化内部的数据结构；做好准备之后，KVM 模块检测系统当前的 CPU，然后打开 CPU 控制寄存器 CR4 中的虚拟化模式开关，并通过执行 VMXON 指令将宿主操作系统（包括KVM模块本身）置于 CPU 执行模式的虚拟化模式中的根模式；最后，KVM 模块创建特殊设备文件 /dev/kvm 并等待来自用户空间的命令。接下来，虚拟机的创建和运行将是一个用户空间的应用程序（QEMU）和 KVM 模块相互配合的过程。
/dev/kvm 这个设备可以被当作一个标准的字符设备，KVM 模块与用户空间 QEMU 的通信接口主要是一系列针对这个特殊设备文件的loctl调用。当然，每个虚拟客户机针对 /dev/kvm 文件的最重要的 loctl 调用就是“创建虚拟机”。

在这里，“创建虚拟机”可以理解成 KVM 为了某个特定的虚拟客户机（用户空间程序创建并初始化）创建对应的内核数据结构。同时，KVM 还会返回一个文件句柄来代表所创建的虚拟机。针对该文件句柄的 loctl 调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真实内存物理地址的映射关系，再比如创建多个可供运行的虚拟处理器（vCPU）。

同样，KVM 模块会为每一个创建出来的虚拟处理器生成对应的文件句柄，对虚拟处理器相应的文件句柄进行相应的loctl调用，就可以对虚拟处理器进行管理。
针对虚拟处理器的最重要的 loctl 调用就是“执行虚拟处理器”。通过它，用户空间准备好的虚拟机在 KVM 模块的支持下，被置于虚拟化模式中的非根模式下，开始执行二进制指令。在非根模式下，所有敏感的二进制指令都会被处理器捕捉到，处理器在保存现场之后自动切换到根模式，由KVM 决定如何进一步处理（要么由 KVM 模块直接处理，要么返回用户空间交由用户空间程序处理）。
除了处理器的虚拟化，内存虚拟化也是由 KVM 模块实现的，包括前面提到的使用硬件提供的 EPT 特性，通过两级转换实现客户机虚拟地址到宿主机物理地址之间的转换。
处理器对设备的访问主要是通过 I/O 指令和 MMIO，其中 I/O 指令会被处理器直接截获，MMIO会通过配置内存虚拟化来捕捉。但是，外设的模拟一般不由 KVM 模块负责。一般来说，只有对性能要求比较高的虚拟设备才会由 KVM 内核模块来直接负责，比如虚拟中断控制器和虚拟时钟，这样可以大量减少处理器模式切换的开销。而大部分的输入输出设备交给下面将要介绍的用户态程序 QEMU 来负责。

## 2.qemu

### qemu 既是模拟器又是虚拟机

作为模拟器：其可模拟多种不同的处理器硬件架构，主要通过二进制指令翻译，先将客户机的指令通过 qemu TCG（Tiny code generator）翻译成 qemu 中间代码，再将其翻译成目标宿主机代码。（缺点：性能较低，优点：跨平台）

作为虚拟机：其提供完整的 CPU虚拟化，内存虚拟化，设备虚拟化。

### 与 KVM 的结合

除了二进制翻译的方式，QEMU 也能与基于硬件虚拟化的 Xen、KVM结合，为它们提供客户机的设备模拟。通过与 KVM 的密切结合，让虚拟化的性能提升得非常高，在真实的企业级虚拟化场景中发挥重要作用，所以我们通常提及 KVM 虚拟化时就会说 “QEMU/KVM” 这样的软件栈。

早期的 KVM 开发者们为了简化软件架构和代码重用，根据 KVM 特性在 QEMU 的基础上进行了修改（当然这部分修改已经合并回 QEMU 的主干代码，故现在的 QEMU 已原生支持 KVM 虚拟化特性）

从图2-8可以看出，每一个虚拟客户机在宿主机中就体现为一个 QEMU 进程，而客户机的每一个虚拟 CPU 就是一个 QEMU 线程。虚拟机运行期间，QEMU 会通过 KVM 模块提供的系统调用进入内核，由 KVM 模块负责将虚拟机置于处理器的特殊模式下运行。遇到虚拟机进行I/O操作时，KVM 模块会从上次的系统调用出口处返回 QEMU，由 QEMU 来负责解析和模拟这些设备。

从 QEMU 角度来看，也可以说 QEMU 使用了 KVM 模块的虚拟化功能，为自己的虚拟机提供硬件虚拟化的加速，从而极大地提高了虚拟机的性能。除此之外，虚拟机的配置和创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对虚拟机的特殊技术（如：动态迁移），都是由 QEMU 自己实现的。

QEMU 除了提供完全模拟的设备（如：e1000网卡、IDE磁盘等）以外，还支持 virtio 协议的设备模拟。virtio 是一个沟通客户机前端设备与宿主机上设备后端模拟的比较高性能的协议，在前端客户机中需要安装相应的 virtio-blk、virtio-scsi、virtio-net 等驱动，而 QEMU 就实现了 virtio 的虚拟化后端。

## 3.组件及管理工具

在 KVM 虚拟化的软件栈中，毋庸置疑的是 KVM 内核模块与 QEMU 用户态程序是处于最核心的位置，有了它们就可通过 qemu 命令行操作实现完整的虚拟机功能，本书中多数的实践范例正是通过 qemu 命令行来演示的。然而，在实际的云计算的虚拟化场景中，为了更高的性能或者管理的方便性，还有很多的软件可以作为 KVM 虚拟化实施中的组件，这里简单介绍其中的几个。

### 与QEMU/KVM结合的组件

**1. vhost-net**
vhost-net 是Linux 内核中的一个模块，它用于替代 QEMU 中的 virtio-net 用户态的 virtio 网络的后端实现。使用 vhost-net 时，还支持网卡的多队列，整体来说会让网络性能得到较大提高。
**2. Open vSwitch**
Open vSwitch 是一个高质量的、多层虚拟交换机，使用开源 Apache2.0 许可协议，主要用可移植性强的 C 语言编写的。它的目的是让大规模网络自动化可以通过编程扩展，同时仍然支持标准的管理接口和协议（例如 NetFlow、sFlow、SPAN、RSPAN、CLI、LACP、802.1ag）。同时也提供了对 OpenFlow 协议的支持，用户可以使用任何支持 OpenFlow 协议的控制器对 OVS 进行远程管理控制。Open vSwitch 被设计为支持跨越多个物理服务器的分布式环境，类似于 VMware 的 vNetwork 分布式 vswitch 或 Cisco Nexus 1000 V。Open vSwitch 支持多种虚拟化技术，包括 Xen/XenServer、KVM 和 VirtualBox。在KVM虚拟化中，要实现软件定义网络（SDN），那么 Open vSwitch 是一个非常好的开源选择。
**3. DPDK**
DPDK 全称是 Data Plane Development Kit，最初是由 Intel 公司维护的数据平面开发工具集，为 Intel x86 处理器架构下用户空间高效的数据包处理提供库函数和驱动的支持，现在也是一个完全独立的开源项目，它还支持 POWER 和 ARM 处理器架构。不同于 Linux 系统以通用性设计为目的，它专注于网络应用中数据包的高性能处理。具体体现在 DPDK 应用程序是运行在用户空间上，利用自身提供的数据平面库来收发数据包，绕过了 Linux 内核协议栈对数据包处理过程。其优点是：性能高、用户态开发、出故障后易恢复。在 KVM 架构中，为了达到非常高的网络处理能力（特别是小包处理能力），可以选择 DPDK 与 QEMU 中的 vhost-user 结合起来使用。
**4. SPDK**
SPDK 全称是 Storage Performance Development Kit，它可为编写高性能、可扩展的、用户模式的存储程序提供一系列工具及开发库。它与 DPDK 非常类似，其主要特点是：将驱动放到用户态从而实现零拷贝、用轮询模式替代传统的中断模式、在所有的 I/O 链路上实现无锁设计，这些设计会使其性能比较高。在 KVM 中需要非常高的存储 I/O 性能时，可以将 QEMU 与 SPDK 结合使用。
**5. Ceph**
Ceph 是 Linux 上一个著名的分布式存储系统，能够在维护 POSIX 兼容性的同时加入复制和容错功能。Ceph由储存管理器（Object storage cluster对象存储集群，即OSD守护进程）、集群监视器（Ceph Monitor）和元数据服务器（Metadata server cluster，MDS）构成。其中，元数据服务器 MDS 仅仅在客户端通过文件系统方式使用 Ceph 时才需要。当客户端通过块设备或对象存储使用 Ceph 时，可以没有 MDS。Ceph 支持3种调用接口：对象存储，块存储，文件系统挂载。在 libvirt 和 QEMU 中都有 Ceph 的接口，所以 Ceph 与 KVM 虚拟化集成是非常容易的。在 OpenStack 的云平台解决方案中，Ceph 是一个非常常用的存储后端。
**6. libguestfs**
libguestfs 是用于访问和修改虚拟机的磁盘镜像的一组工具集合。libguestfs 提供了访问和编辑客户机中的文件、脚本化修改客户机中的信息、监控磁盘使用和空闲的统计信息、P2V、V2V、创建客户机、克隆客户机、备份磁盘内容、格式化磁盘、调整磁盘大小等非常丰富的功能。libguestfs 还提供了共享库，可以在C/C++、Python等编程语言中对其进行调用。libguestfs 不需要启动KVM客户机就可以对磁盘镜像进行管理，功能强大且非常灵活，是管理 KVM 磁盘镜像的首选工具。

### KVM上层管理工具

一个成熟的虚拟化解决方案离不开良好的管理和运维工具，部署、运维、管理的复杂度与灵活性是企业实施虚拟化时重点考虑的问题。KVM目前已经有从libvirt API、virsh命令行工具到OpenStack云管理平台等一整套管理工具，尽管与老牌虚拟化巨头VMware提供的商业化虚拟化管理工具相比在功能和易用性上有所差距，但KVM这一整套管理工具都是API化的、开源的，在使用的灵活性以及对其做二次开发的定制化方面仍有一定优势。根据笔者的实践经验，本节给大家概括性地介绍KVM软件栈中常见的几个管理运维工具，在第4章将会详细介绍相关内容。
**1. libvirt**
libvirt是使用最广泛的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准，本节后部分介绍的其他工具都是基于libvirt的API来实现的。作为通用的虚拟化API，libvirt不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox等其他虚拟化方案。
**2. virsh**
virsh是一个常用的管理KVM虚拟化的命令行工具，对于系统管理员在单个宿主机上进行运维操作，virsh命令行可能是最佳选择。virsh是用C语言编写的一个使用libvirt API 的虚拟化管理工具，其源代码也是在libvirt这个开源项目中的。
**3. virt-manager**
virt-manager是专门针对虚拟机的图形化管理软件，底层与虚拟化交互的部分仍然是调用libvirt API来操作的。virt-manager除了提供虚拟机生命周期（包括：创建、启动、停止、打快照、动态迁移等）管理的基本功能，还提供性能和资源使用率的监控，同时内置了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、CentOS、Fedora等操作系统上是非常流行的虚拟化管理软件，在管理的机器数量规模较小时，virt-manager是很好的选择。因其图形化操作的易用性，成为新手入门学习虚拟化操作的首选管理软件。
**4. OpenStack**
OpenStack是一个开源的基础架构即服务（IaaS）云计算管理平台，可用于构建共有云和私有云服务的基础设施。OpenStack是目前业界使用最广泛的功能最强大的云管理平台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如：对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等。OpenStack仍然使用libvirt API来完成对底层虚拟化的管理。

## 小结





